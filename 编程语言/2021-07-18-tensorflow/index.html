<!DOCTYPE html><html lang="zh-CN"><head><!-- hexo injector head_begin start --><link href="/css/tag-common/index.css" rel="stylesheet"/><!-- hexo injector head_begin end --><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#6200ee"><meta name="author" content="larry"><meta name="copyright" content="larry"><meta name="generator" content="Hexo 5.4.0"><meta name="theme" content="hexo-theme-yun"><title>tensorflow | 拉瑞君の小窝</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.25/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_dxory92pb0h.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link class="aplayer-style-marker" rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script class="aplayer-script-marker" src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" defer></script><script class="meting-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js" defer></script><script>document.addEventListener(
  "pjax:success",
  function() {
    if (window.aplayers) {
      loadMeting();
    }
  },
  !1
);
</script><link id="light-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism.css" media="(prefers-color-scheme: light)"><link id="dark-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism-tomorrow.css" media="(prefers-color-scheme: dark)"><link rel="icon" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#6200ee"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"larrystd.github.io","root":"/","title":"拉瑞君の小窝","version":"1.6.3","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"algolia":{"appID":"CJXXAGRCYN","apiKey":"ae1966d2aeab22bf9335679f45d2cd9a","indexName":"my-hexo-blog","hits":{"per_page":8}},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><link rel="alternate" href="/atom.xml" title="拉瑞君の小窝" type="application/atom+xml"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-1LL0D86CY9"></script><script>if (CONFIG.hostname === location.hostname) {
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-1LL0D86CY9');
}</script><script data-ad-client="ca-pub-2245427233262012" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(function(){
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})();</script><!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-M9KWR9L');</script><!-- End Google Tag Manager --><meta name="description" content="tensorflow2.0基本知识创建和操作Tensor import tensorflow as tf  print (tf.__version__)    x &#x3D; tf.constant(range(12)) print(x.shape)  # (12,)  x  &lt;tf.Tensor: id&#x3D;1, shape&#x3D;(12,), dtype&#x3D;int32">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow">
<meta property="og:url" content="https://larrystd.github.io/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/2021-07-18-tensorflow/index.html">
<meta property="og:site_name" content="拉瑞君の小窝">
<meta property="og:description" content="tensorflow2.0基本知识创建和操作Tensor import tensorflow as tf  print (tf.__version__)    x &#x3D; tf.constant(range(12)) print(x.shape)  # (12,)  x  &lt;tf.Tensor: id&#x3D;1, shape&#x3D;(12,), dtype&#x3D;int32">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-07-17T16:00:00.000Z">
<meta property="article:modified_time" content="2021-07-17T16:00:00.000Z">
<meta property="article:author" content="larry">
<meta property="article:tag" content="python">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="larry"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="larry"><span class="site-author-status" title="Looking for you.">🌑</span></a><div class="site-author-name"><a href="/about/">larry</a></div><a class="site-name" href="/about/site.html">拉瑞君の小窝</a><sub class="site-subtitle"></sub><div class="site-desciption">每天都是新的一天呢</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">86</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">13</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">42</span></a></div><a class="site-state-item hty-icon-button" href="/about/#comment" title="留言板"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-clipboard-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/larrystd" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.zhihu.com/people/bu-qu-dou-yin-bu-gai-ming" title="知乎" target="_blank" style="color:#0084FF"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhihu-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="Venray.Kong@outlook.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://travellings.link" title="Travelling" target="_blank" style="color:var(--hty-text-color)"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-train-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a><a class="links-item hty-icon-button" href="/girls/" title="喜欢的女孩子" style="color:hotpink"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-women-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#tensorflow2-0"><span class="toc-number">1.</span> <span class="toc-text">tensorflow2.0</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86"><span class="toc-number">1.1.</span> <span class="toc-text">基本知识</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90"><span class="toc-number">1.3.</span> <span class="toc-text">文本分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensorflow-1-0"><span class="toc-number">2.</span> <span class="toc-text">tensorflow 1.0</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://larrystd.github.io/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/2021-07-18-tensorflow/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="larry"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="拉瑞君の小窝"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">tensorflow<a class="post-edit-link" href="https://github.com/larrystd/larrystd.github.io/tree/hexo/source/_posts/编程语言/2021-07-18-tensorflow.md" target="_blank" title="编辑" rel="noopener"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-edit-line"></use></svg></a></h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2021-07-18 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-18T00:00:00+08:00">2021-07-18</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">3k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">16m</span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/language/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">language</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/python/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">python</span></a><a class="tag-item" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">机器学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#6200ee;"><h3 id="tensorflow2-0"><a href="#tensorflow2-0" class="headerlink" title="tensorflow2.0"></a>tensorflow2.0</h3><h4 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h4><p>创建和操作Tensor</p>
<pre class="language-py" data-language="py"><code class="language-py">import tensorflow as tf 
print (tf.__version__)



x &#x3D; tf.constant(range(12))
print(x.shape)

# (12,)

x

&lt;tf.Tensor: id&#x3D;1, shape&#x3D;(12,), dtype&#x3D;int32, numpy&#x3D;array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype&#x3D;int32)&gt;

X &#x3D; tf.reshape(x,(3,4))
&lt;tf.Tensor: id&#x3D;2, shape&#x3D;(3, 4), dtype&#x3D;int32, numpy&#x3D;
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])&gt;


tf.zeros((2,3,4))

Y &#x3D; tf.constant([[2,1,4,3],[1,2,3,4],[4,3,2,1]])

# 正态分布
tf.random.normal(shape&#x3D;[3,4], mean&#x3D;0, stddev&#x3D;1)

# 按元素乘法：
X * Y
# 按元素指数运算
Y &#x3D; tf.cast(Y, tf.float32)
tf.exp(Y)


# 矩阵乘法
Y &#x3D; tf.cast(Y, tf.int32)
tf.matmul(X, tf.transpose(Y))

# 按列连接
tf.concat([X,Y],axis &#x3D; 0)
# 按行连接
tf.concat([X,Y],axis &#x3D; 1)

# 对tensor中的所有元素求和得到只有一个元素的tensor。
tf.reduce_sum(X)

# 范数
X &#x3D; tf.cast(X, tf.float32)
tf.norm(X)

# 支持切片

X[1:3]

X &#x3D; tf.Variable(X)

&lt;tf.Variable &#39;Variable:0&#39; shape&#x3D;(3, 4) dtype&#x3D;float32, numpy&#x3D;
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  9.,  7.],
       [ 8.,  9., 10., 11.]], dtype&#x3D;float32)&gt;

X[1,2].assign(9) # 重新赋值

# 即使像Y &#x3D; X + Y这样的运算，我们也会新开内存，然后将Y指向新内存。

X &#x3D; tf.Variable(X)
Y &#x3D; tf.cast(Y, dtype&#x3D;tf.float32)

before &#x3D; id(Y)
Y &#x3D; Y + X
id(Y) &#x3D;&#x3D; before

False</code></pre>

<span id="more"></span>

<p>与numpy转换</p>
<pre class="language-none"><code class="language-none">import numpy as np

P &#x3D; np.ones((2,3))
D &#x3D; tf.constant(P)

np.array(D)</code></pre>

<p>tensor与numpy转换 </p>
<pre class="language-py" data-language="py"><code class="language-py">import numpy as np
p &#x3D; np.ones((2,3))
d &#x3D; torch.from_numpy(p)

d.numpy()</code></pre>

<p>自动求梯度</p>
<p>tensorflow2.0提供的GradientTape来自动求梯度。</p>
<pre class="language-py" data-language="py"><code class="language-py">x &#x3D; tf.reshape(tf.Variable(range(4), dtype&#x3D;tf.float32),(4,1))
x

&lt;tf.Tensor: id&#x3D;10, shape&#x3D;(4, 1), dtype&#x3D;float32, numpy&#x3D;
array([[0.],
       [1.],
       [2.],
       [3.]], dtype&#x3D;float32)&gt;

with tf.GradientTape() as t:
    t.watch(x)
    y &#x3D; 2 * tf.matmul(tf.transpose(x), x)
    
dy_dx &#x3D; t.gradient(y, x)
dy_dx

&lt;tf.Tensor: id&#x3D;30, shape&#x3D;(4, 1), dtype&#x3D;float32, numpy&#x3D;
array([[ 0.],
       [ 4.],
       [ 8.],
       [12.]], dtype&#x3D;float32)&gt;


def f(a):
    b &#x3D; a * 2
    while tf.norm(b) &lt; 1000:
        b &#x3D; b * 2
    if tf.reduce_sum(b) &gt; 0:
        c &#x3D; b
    else:
        c &#x3D; 100 * b
    return c
a &#x3D; tf.random.normal((1,1),dtype&#x3D;tf.float32)
with tf.GradientTape() as t:
    t.watch(a)
    c &#x3D; f(a)
t.gradient(c,a) &#x3D;&#x3D; c&#x2F;a

&lt;tf.Tensor: id&#x3D;201, shape&#x3D;(1, 1), dtype&#x3D;bool, numpy&#x3D;array([[ True]])&gt;</code></pre>

<h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><pre class="language-py" data-language="py"><code class="language-py">import tensorflow as tf
print(tf.__version__)
from matplotlib import pyplot as plt
import random

# 生成数据集
num_inputs &#x3D; 2
num_examples &#x3D; 1000
true_w &#x3D; [2, -3.4]
true_b &#x3D; 4.2
features &#x3D; tf.random.normal((num_examples, num_inputs),stddev &#x3D; 1)
labels &#x3D; true_w[0] * features[:,0] + true_w[1] * features[:,1] + true_b
labels +&#x3D; tf.random.normal(labels.shape,stddev&#x3D;0.01)

# 读取数据集
def data_iter(batch_size, features, labels):
    num_examples &#x3D; len(features)
    indices &#x3D; list(range(num_examples))
    random.shuffle(indices)
    # 依次读取一个batch
    for i in range(0, num_examples, batch_size):
        j &#x3D; indices[i: min(i+batch_size, num_examples)]
        # tf.gather抽取出params的第axis维度上在indices里面所有的index
        yield tf.gather(features, axis&#x3D;0, indices&#x3D;j), tf.gather(labels, axis&#x3D;0, indices&#x3D;j)

# 初始化模型参数

w &#x3D; tf.Variable(tf.random.normal((num_inputs, 1), stddev&#x3D;0.01))
b &#x3D; tf.Variable(tf.zeros((1,)))

# 定义模型
def linreg(X, w, b):
    return tf.matmul(X, w) + b
# 损失函数
def squared_loss(y_hat, y):
    return (y_hat - tf.reshape(y, y_hat.shape)) ** 2 &#x2F;2
# 优化函数
def sgd(params, lr, batch_size, grads):
    &quot;&quot;&quot;Mini-batch stochastic gradient descent.&quot;&quot;&quot;
    for i, param in enumerate(params):
        # 更新参数
        # assign_sub 变量 ref 减去 value值，即 ref &#x3D; ref - value 并赋值
        param.assign_sub(lr * grads[i] &#x2F; batch_size)

# 模型训练
lr &#x3D; 0.03
num_epochs &#x3D; 3
net &#x3D; linreg
loss &#x3D; squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        with tf.GradientTape() as t:
            t.watch([w,b])
            l &#x3D; tf.reduce_sum(loss(net(X, w, b), y))
        # 自动求梯度
        # 二维的向量，分别表示[w,b]的梯度
        grads &#x3D; t.gradient(l, [w, b])
        sgd([w, b], lr, batch_size, grads)
    train_l &#x3D; loss(net(features, w, b), labels)
    print(&#39;epoch %d, loss %f&#39; % (epoch + 1, tf.reduce_mean(train_l)))</code></pre>

<p>使用模块写线性回归</p>
<pre class="language-py" data-language="py"><code class="language-py">
import tensorflow as tf

# 生成数据集
num_inputs &#x3D; 2
num_examples &#x3D; 1000
true_w &#x3D; [2, -3.4]
true_b &#x3D; 4.2
features &#x3D; tf.random.normal(shape&#x3D;(num_examples, num_inputs), stddev&#x3D;1)
labels &#x3D; true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels +&#x3D; tf.random.normal(labels.shape, stddev&#x3D;0.01)


from tensorflow import data as tfdata

batch_size &#x3D; 10
# 将训练数据的特征和标签组合
dataset &#x3D; tfdata.Dataset.from_tensor_slices((features, labels))
# 随机读取小批量
dataset &#x3D; dataset.shuffle(buffer_size&#x3D;num_examples) 
dataset &#x3D; dataset.batch(batch_size)
data_iter &#x3D; iter(dataset) # 迭代器读取每个batch

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow import initializers as init

# 模型容器
model &#x3D; keras.Sequential()
model.add(layers.Dense(1, kernel_initializer&#x3D;init.RandomNormal(stddev&#x3D;0.01)))

# 损失函数
from tensorflow import losses
loss &#x3D; losses.MeanSquaredError()

# 优化器
from tensorflow.keras import optimizers
trainer &#x3D; optimizers.SGD(learning_rate&#x3D;0.03)

# 训练模型
num_epochs &#x3D; 3
for epoch in range(1, num_epochs + 1):
    for (batch, (X, y)) in enumerate(dataset):
        # 记录梯度
        with tf.GradientTape() as tape:
            l &#x3D; loss(model(X, training&#x3D;True), y)
        # 获取变量梯度
        grads &#x3D; tape.gradient(l, model.trainable_variables)
        # 更新参数
        # trainer为优化器
        trainer.apply_gradients(zip(grads, model.trainable_variables))
    
    l &#x3D; loss(model(features), labels)
    print(&#39;epoch %d, loss: %f&#39; % (epoch, l))</code></pre>
<p>通过调用<code>tensorflow.GradientTape</code>记录动态图梯度，执行<code>tape.gradient</code>获得动态图中各变量梯度。通过 <code>model.trainable_variables</code> 找到需要更新的变量，并用 <code>trainer.apply_gradients</code> 更新权重，完成一步训练。</p>
<h4 id="文本分析"><a href="#文本分析" class="headerlink" title="文本分析"></a>文本分析</h4><pre class="language-py" data-language="py"><code class="language-py">import collections
from collections import defaultdict
import os
import random
import tarfile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import preprocessing
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import text_to_word_sequence, one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import sys
import time
import os
sys.path.append(&quot;..&quot;)
import d2lzh_tensorflow2 as d2l
print(tf.test.gpu_device_name())
DATA_ROOT &#x3D; &quot;..&#x2F;..&#x2F;data&quot;

import torchtext.vocab as Vocab

port tensorflow as tf
AUTO &#x3D; tf.data.experimental.AUTOTUNE
physical_devices &#x3D; tf.config.list_physical_devices(&#39;GPU&#39;)
tf.config.experimental.set_memory_growth(physical_devices[0], enable&#x3D;True)

#数据放入data目录下，代码解压速度较慢，如果不想用代码解压，也可直接手动解压，跳过这一步
import os
path &#x3D; os.getcwd()#返回当前进程的工作目录
a_path &#x3D; os.path.abspath(os.path.join(path, &quot;..&#x2F;..&#x2F;data&#x2F;aclImdb_v1.tar.gz&quot;))
with tarfile.open(a_path, &#39;r&#39;) as f:
    f.extractall(DATA_ROOT)

def read_imdb(folder&#x3D;&#39;train&#39;, data_root&#x3D;&quot;..&#x2F;..&#x2F;data&#x2F;aclImdb&#x2F;&quot;):
    data &#x3D; []
    for label in [&#39;pos&#39;, &#39;neg&#39;]:
        folder_name &#x3D; os.path.join(data_root, folder, label)
        for file in (os.listdir(folder_name)):
            with open(os.path.join(folder_name, file), &#39;rb&#39;) as f:
                review &#x3D; f.read().decode(&#39;utf-8&#39;).replace(&#39;\n&#39;, &#39;&#39;).lower()
                data.append([review, 1 if label &#x3D;&#x3D; &#39;pos&#39; else 0])
    random.shuffle(data)
    return data
# 读取数据
train_data, test_data &#x3D; read_imdb(&#39;train&#39;), read_imdb(&#39;test&#39;)

# 预处理
def get_tokenized_imdb(data):
    &quot;&quot;&quot;
    data: list of [string, label]
    返回: 每一条评论的单词所组成的列表
    &quot;&quot;&quot;
    def tokenizer(text):
        #基于空格进行分词，并都转换为小写
        return [tok.lower() for tok in text.split(&#39; &#39;)]

    return [tokenizer(review) for review, _ in data]

text &#x3D; get_tokenized_imdb(train_data)

# Counter对列表中的单词进行计数，并返回一个字典
counter &#x3D; collections.Counter([tk for st in text for tk in st])
# vocab是一个字典，键表示单词，值表示单词出现的频率
vocab &#x3D; &#123;w: freq for w, freq in counter.most_common() if freq &gt; 5&#125;

def get_vocab_imdb(data):
    tokenized_data &#x3D; get_tokenized_imdb(data)
    #counter已经创建了一个词典,统计了每个词出现的频率
    counter &#x3D; collections.Counter([tk for st in tokenized_data for tk in st])
    return Vocab.Vocab(counter, min_freq&#x3D;5)
    #text值保存counter中出现频率大于等于五的词
#     text &#x3D; &#123;w: freq for w, freq in counter.most_common() if freq &gt;&#x3D; 5&#125;
#     vocab &#x3D;&#123;index:word for word,index in enumerate(text.keys())&#125;
#     return vocab
    #return Vocab.Vocab(counter, min_freq&#x3D;5)


vocab &#x3D; get_vocab_imdb(train_data)
&#39;# words in vocab:&#39;, len(vocab)

#在此处使用tensorflow2的填充函数进行填充
def preprocess_imdb(data, vocab):  # 本函数已保存在d2lzh_tensorflow2包中方便以后使用
    max_l &#x3D; 500

    # 将每条评论通过截断或者补0，使得长度变成500
    def pad(x):
        return x[:max_l] if len(x) &gt; max_l else x + [0] * (max_l - len(x))
    
    #tokenized_data为一个二维的列表,里面有我们分好的词
    tokenized_data &#x3D; get_tokenized_imdb(data)
    
     #将每个词转换为词索引并进行截断或补0
    features &#x3D; tf.Variable([pad([vocab[word] for word in words] ) for words in tokenized_data])
    labels &#x3D; tf.Variable([score for _, score in data])
    return features, labels

data, label &#x3D; preprocess_imdb(train_data, vocab)

batch_size &#x3D; 64

# 训练集
train_set &#x3D; (tf.data.Dataset.from_tensor_slices(
    ((preprocess_imdb(train_data, vocab))))
    .repeat()
    .shuffle(2048)
    .batch(batch_size)
    .prefetch(AUTO))
# 测试集
test_set &#x3D; (tf.data.Dataset.from_tensor_slices(
    ((preprocess_imdb(test_data, vocab))))
    .shuffle(2048)
    .batch(batch_size)
    .prefetch(AUTO))

#因为tensorflow并没有像pytorch，mxnet关于glove接口的api，所以必须要重写一个

def load_embedding_from_disks(glove_filename, with_indexes&#x3D;True):
    &quot;&quot;&quot;
    Read a GloVe txt file. If &#96;with_indexes&#x3D;True&#96;, we return a tuple of two dictionnaries
    &#96;(word_to_index_dict, index_to_embedding_array)&#96;, otherwise we return only a direct 
    &#96;word_to_embedding_dict&#96; dictionnary mapping from a string to a numpy array.
    &quot;&quot;&quot;
    if with_indexes:
        word_to_index_dict &#x3D; dict()
        index_to_embedding_array &#x3D; []
        index_to_word_dict &#x3D; dict()
        word_to_embedding &#x3D; dict()
    else:
        word_to_embedding_dict &#x3D; dict()

    
    with open(glove_filename, &#39;r&#39;,encoding&#x3D;&#39;utf-8&#39;) as glove_file:
        for (i, line) in enumerate(glove_file):
            
            split &#x3D; line.split(&#39; &#39;)
            
            word &#x3D; split[0]
            
            representation &#x3D; split[1:]
            representation &#x3D; np.array(
                [float(val) for val in representation]
            )
            
            if with_indexes:
                word_to_index_dict[word] &#x3D; i
                index_to_word_dict[i] &#x3D; word
                word_to_embedding[word] &#x3D; representation
                index_to_embedding_array.append(representation)
            else:
                word_to_embedding_dict[word] &#x3D; representation

    _WORD_NOT_FOUND &#x3D; [0.0]* len(representation)  # Empty representation for unknown words.
    if with_indexes:
        _LAST_INDEX &#x3D; i + 1
        word_to_index_dict &#x3D; defaultdict(lambda: _LAST_INDEX, word_to_index_dict)
        index_to_embedding_array &#x3D; np.array(index_to_embedding_array + [_WORD_NOT_FOUND])
        return word_to_index_dict, index_to_embedding_array,index_to_word_dict,word_to_embedding
    else:
        word_to_embedding_dict &#x3D; defaultdict(lambda: _WORD_NOT_FOUND)
        return word_to_embedding_dict
    
word_to_index, index_to_embedding, index_to_word,word_to_embedding &#x3D; load_embedding_from_disks(&quot;C:&#x2F;Users&#x2F;HP&#x2F;dive into d2l&#x2F;code&#x2F;chapter10_natural-language-processing&#x2F;embeddings&#x2F; GloVe.6B&#x2F;glove.6B.50d.txt&quot;, with_indexes&#x3D;True)

def get_weights(vocab, word_to_embedding,embedding_dim,word_to_index,index_to_embedding):
    &quot;&quot;&quot;从预训练好的vocab中提取出words对应的词向量&quot;&quot;&quot;
    embedding_matrix &#x3D; np.zeros((len(vocab), embedding_dim))
#     embedding_matrix &#x3D; np.zeros((len(vocab), embedding_dim))
    for index, word in enumerate(vocab.itos):
        if word in word_to_embedding.keys():
            embedding_matrix[index] &#x3D; index_to_embedding[index]
    return embedding_matrix
embedding_matrix &#x3D; get_weights(vocab,word_to_embedding,50,word_to_index,index_to_embedding)
# net.embedding.set_weights([embedding_matrix])
# net.trainable &#x3D; False

embed_size, num_hiddens, max_len &#x3D; 50, 100, 500
num_epochs &#x3D; 5

model &#x3D; tf.keras.Sequential([
    layers.Embedding(len(vocab), embed_size,weights&#x3D;[embedding_matrix],input_length&#x3D;500),
    layers.Bidirectional(layers.LSTM(num_hiddens)),
    tf.keras.layers.Dense(2,activation&#x3D;&#39;softmax&#39;)
])

model.layers[0].trainable &#x3D; False
model.summary()

model.compile(tf.keras.optimizers.Adam(0.01),
            loss&#x3D;&#39;sparse_categorical_crossentropy&#39;,
            metrics&#x3D;[&#39;sparse_categorical_accuracy&#39;])

model.fit(
    train_set,
    steps_per_epoch&#x3D;data.shape[0]&#x2F;&#x2F;batch_size,
    validation_data&#x3D; test_set,
    epochs&#x3D;5
    )
</code></pre>

<h3 id="tensorflow-1-0"><a href="#tensorflow-1-0" class="headerlink" title="tensorflow 1.0"></a>tensorflow 1.0</h3><p>一个TensorFlow图由下面几个部分组成</p>
<ul>
<li>占位符变量（Placeholder）用来改变图的输入。</li>
<li>模型变量（Model）将会被优化，使得模型表现得更好。</li>
<li>模型本质上就是一些数学函数，它根据Placeholder和模型的输入变量来计算一些输出。</li>
<li>一个cost度量用来指导变量的优化。</li>
<li>一个优化策略会更新模型的变量。</li>
<li>另外，TensorFlow图也包含了一些调试状态，比如用TensorBoard打印log数据</li>
</ul>
<p>Placeholder是作为图的输入，每次我们运行图的时候都可能会改变它们。</p>
<pre class="language-py" data-language="py"><code class="language-py"># 数据类型设置为float32，形状设为[None, img_size_flat]，None代表tensor可能保存着任意数量的图像，每张图象是一个长度为img_size_flat的向量。
x &#x3D; tf.placeholder(tf.float32, [None, img_size_flat])
y_true_cls &#x3D; tf.placeholder(tf.int64, [None])

# 变量的形状是[None, num_classes]，这代表着它保存了任意数量的标签，每个标签是长度为num_classes的向量，本例中长度为10。
y_true &#x3D; tf.placeholder(tf.float32, [None, num_classes])

weights &#x3D; tf.Variable(tf.zeros([img_size_flat, num_classes]))

logits &#x3D; tf.matmul(x, weights) + biases
y_pred &#x3D; tf.nn.softmax(logits)
y_pred_cls &#x3D; tf.argmax(y_pred, axis&#x3D;1)

cross_entropy &#x3D; tf.nn.softmax_cross_entropy_with_logits_v2(logits&#x3D;logits,
                                                           labels&#x3D;y_true)
cost &#x3D; tf.reduce_mean(cross_entropy)
optimizer &#x3D; tf.train.GradientDescentOptimizer(learning_rate&#x3D;0.5).minimize(cost)

correct_prediction &#x3D; tf.equal(y_pred_cls, y_true_cls)
accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# 会话
session &#x3D; tf.Session()
# 会对weights和biases变量初始化
session.run(tf.global_variables_initializer())、

batch_size &#x3D; 100
def optimize(num_iterations):
    for i in range(num_iterations):
        # Get a batch of training examples.
        # x_batch now holds a batch of images and
        # y_true_batch are the true labels for those images.
        x_batch, y_true_batch, _ &#x3D; data.random_batch(batch_size&#x3D;batch_size)
        
        # Put the batch into a dict with the proper names
        # for placeholder variables in the TensorFlow graph.
        # Note that the placeholder for y_true_cls is not set
        # because it is not used during training.
        feed_dict_train &#x3D; &#123;x: x_batch,
                           y_true: y_true_batch&#125;

        # Run the optimizer using this batch of training data.
        # TensorFlow assigns the variables in feed_dict_train
        # to the placeholder variables and then runs the optimizer.
        session.run(optimizer, feed_dict&#x3D;feed_dict_train)</code></pre>

<p>会话是用来执行定义好的运算，会话拥有并管理TensorFlow程序运行时的所有资源，所有计算完成之后需要关闭会话来帮助系统回收资源。用户通过placeholder定义占位符并构建完整Graph后，利用Session实例.run将训练/测试数据注入到图中，驱动任务的实际运行。</p>
<p>TensorFlow的设计处处体现了一个延后计算的思想。从而可以做更多静态分析，图优化，从而达到高运行效率。而pytorch图动态每一次前向时构建graph，反向时销毁。</p>
</div></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%9B%E5%A4%A7%E4%BB%B6/2021-07-20-IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/" rel="prev" title="IO多路复用"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">IO多路复用</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/2021-07-17-NLP%E4%B8%80%E4%BA%9B%E5%B7%A5%E5%85%B7/" rel="next" title="NLP tools"><span class="post-nav-text">NLP tools</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>如果您有任何关于博客内容的相关讨论，欢迎前往 <a href="https://github.com/YunYouJun/yunyoujun.github.io/discussions" target="_blank">GitHub Discussions</a> 与我交流。</span><br></div><div id="valine-container"></div><script>Yun.utils.getScript("https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js", () => {
  const valineConfig = {"enable":true,"appId":"K4LElSwpTJaHOOTTU6mNGCyr-gzGzoHsz","appKey":"x3d4Sv6rdTYOECKqkxg9r905","placeholder":"填写邮箱，可以收到回复通知哦～","avatar":null,"pageSize":10,"visitor":false,"highlight":true,"recordIP":false,"enableQQ":true,"meta":["nick","mail","link"],"el":"#valine-container","lang":"zh-cn"}
  valineConfig.path = "/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/2021-07-18-tensorflow/"
  new Valine(valineConfig)
}, window.Valine);</script></div></main><footer class="sidebar-translate" id="footer"><div class="beian"><a rel="noopener" href="https://beian.miit.gov.cn/" target="_blank">萌ICP备666666号</a></div><div class="copyright"><span>&copy; 2020 – 2021 </span><a class="with-love" id="animate" target="_blank" rel="noopener" href="https://sponsors.yunyoujun.cn" title="云游君的赞助者们"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></a><span class="author"> larry</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v5.4.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.6.3</span></div><div class="live_time"><span>本博客已萌萌哒地运行</span><span id="display_live_time"></span><span class="moe-text">(●'◡'●)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2019-04-12T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = " " + passDay + " 天 " + passHour + " 小时 " + passMinute + " 分 " + passSecond + " 秒";
}
blog_live_time();
</script></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#6200ee" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script defer src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script defer src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script defer src="/js/search/algolia-search.js"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div class="algolia-pagination" id="algolia-pagination"></div></div></div></div><!-- hexo injector body_end start --><script src="/js/tag-common/index.js"></script><!-- hexo injector body_end end --></body></html>