<!DOCTYPE html><html lang="zh-CN"><head><!-- hexo injector head_begin start --><link href="/css/tag-common/index.css" rel="stylesheet"/><!-- hexo injector head_begin end --><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#6200ee"><meta name="author" content="larry"><meta name="copyright" content="larry"><meta name="generator" content="Hexo 5.4.0"><meta name="theme" content="hexo-theme-yun"><title>tensorflow | æ‹‰ç‘å›ã®å°çª</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.25/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_dxory92pb0h.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link class="aplayer-style-marker" rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script class="aplayer-script-marker" src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" defer></script><script class="meting-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js" defer></script><script>document.addEventListener(
  "pjax:success",
  function() {
    if (window.aplayers) {
      loadMeting();
    }
  },
  !1
);
</script><link id="light-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism.css" media="(prefers-color-scheme: light)"><link id="dark-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism-tomorrow.css" media="(prefers-color-scheme: dark)"><link rel="icon" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#6200ee"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"larrystd.github.io","root":"/","title":"æ‹‰ç‘å›ã®å°çª","version":"1.6.3","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"æœç´¢...","empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹: ${query}","hits":"æ‰¾åˆ° ${hits} æ¡ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} æ¡ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰"},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"algolia":{"appID":"CJXXAGRCYN","apiKey":"ae1966d2aeab22bf9335679f45d2cd9a","indexName":"my-hexo-blog","hits":{"per_page":8}},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><link rel="alternate" href="/atom.xml" title="æ‹‰ç‘å›ã®å°çª" type="application/atom+xml"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-1LL0D86CY9"></script><script>if (CONFIG.hostname === location.hostname) {
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-1LL0D86CY9');
}</script><script data-ad-client="ca-pub-2245427233262012" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(function(){
  var bp = document.createElement('script');
  var curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https') {
    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else {
    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})();</script><!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-M9KWR9L');</script><!-- End Google Tag Manager --><meta name="description" content="tensorflow2.0åŸºæœ¬çŸ¥è¯†åˆ›å»ºå’Œæ“ä½œTensor import tensorflow as tf  print (tf.__version__)    x &#x3D; tf.constant(range(12)) print(x.shape)  # (12,)  x  &lt;tf.Tensor: id&#x3D;1, shape&#x3D;(12,), dtype&#x3D;int32">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow">
<meta property="og:url" content="https://larrystd.github.io/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/2021-07-18-tensorflow/index.html">
<meta property="og:site_name" content="æ‹‰ç‘å›ã®å°çª">
<meta property="og:description" content="tensorflow2.0åŸºæœ¬çŸ¥è¯†åˆ›å»ºå’Œæ“ä½œTensor import tensorflow as tf  print (tf.__version__)    x &#x3D; tf.constant(range(12)) print(x.shape)  # (12,)  x  &lt;tf.Tensor: id&#x3D;1, shape&#x3D;(12,), dtype&#x3D;int32">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-07-17T16:00:00.000Z">
<meta property="article:modified_time" content="2021-07-17T16:00:00.000Z">
<meta property="article:author" content="larry">
<meta property="article:tag" content="python">
<meta property="article:tag" content="æœºå™¨å­¦ä¹ ">
<meta name="twitter:card" content="summary"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="æ–‡ç« ç›®å½•"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="ç«™ç‚¹æ¦‚è§ˆ"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="larry"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="larry"><span class="site-author-status" title="Looking for you.">ğŸŒ‘</span></a><div class="site-author-name"><a href="/about/">larry</a></div><a class="site-name" href="/about/site.html">æ‹‰ç‘å›ã®å°çª</a><sub class="site-subtitle"></sub><div class="site-desciption">æ¯å¤©éƒ½æ˜¯æ–°çš„ä¸€å¤©å‘¢</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="é¦–é¡µ"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="å½’æ¡£"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">86</span></a></div><div class="site-state-item"><a href="/categories/" title="åˆ†ç±»"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">13</span></a></div><div class="site-state-item"><a href="/tags/" title="æ ‡ç­¾"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">42</span></a></div><a class="site-state-item hty-icon-button" href="/about/#comment" title="ç•™è¨€æ¿"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-clipboard-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/larrystd" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.zhihu.com/people/bu-qu-dou-yin-bu-gai-ming" title="çŸ¥ä¹" target="_blank" style="color:#0084FF"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhihu-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="Venray.Kong@outlook.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://travellings.link" title="Travelling" target="_blank" style="color:var(--hty-text-color)"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-train-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="æˆ‘çš„å°ä¼™ä¼´ä»¬" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a><a class="links-item hty-icon-button" href="/girls/" title="å–œæ¬¢çš„å¥³å­©å­" style="color:hotpink"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-women-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#tensorflow2-0"><span class="toc-number">1.</span> <span class="toc-text">tensorflow2.0</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86"><span class="toc-number">1.1.</span> <span class="toc-text">åŸºæœ¬çŸ¥è¯†</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.</span> <span class="toc-text">çº¿æ€§å›å½’</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90"><span class="toc-number">1.3.</span> <span class="toc-text">æ–‡æœ¬åˆ†æ</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensorflow-1-0"><span class="toc-number">2.</span> <span class="toc-text">tensorflow 1.0</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://larrystd.github.io/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/2021-07-18-tensorflow/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="larry"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="æ‹‰ç‘å›ã®å°çª"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">tensorflow<a class="post-edit-link" href="https://github.com/larrystd/larrystd.github.io/tree/hexo/source/_posts/ç¼–ç¨‹è¯­è¨€/2021-07-18-tensorflow.md" target="_blank" title="ç¼–è¾‘" rel="noopener"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-edit-line"></use></svg></a></h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="åˆ›å»ºæ—¶é—´ï¼š2021-07-18 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-18T00:00:00+08:00">2021-07-18</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="æœ¬æ–‡å­—æ•°"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="æœ¬æ–‡å­—æ•°">3k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="é˜…è¯»æ—¶é•¿"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="é˜…è¯»æ—¶é•¿">16m</span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/language/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">language</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/python/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">python</span></a><a class="tag-item" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">æœºå™¨å­¦ä¹ </span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#6200ee;"><h3 id="tensorflow2-0"><a href="#tensorflow2-0" class="headerlink" title="tensorflow2.0"></a>tensorflow2.0</h3><h4 id="åŸºæœ¬çŸ¥è¯†"><a href="#åŸºæœ¬çŸ¥è¯†" class="headerlink" title="åŸºæœ¬çŸ¥è¯†"></a>åŸºæœ¬çŸ¥è¯†</h4><p>åˆ›å»ºå’Œæ“ä½œTensor</p>
<pre class="language-py" data-language="py"><code class="language-py">import tensorflow as tf 
print (tf.__version__)



x &#x3D; tf.constant(range(12))
print(x.shape)

# (12,)

x

&lt;tf.Tensor: id&#x3D;1, shape&#x3D;(12,), dtype&#x3D;int32, numpy&#x3D;array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype&#x3D;int32)&gt;

X &#x3D; tf.reshape(x,(3,4))
&lt;tf.Tensor: id&#x3D;2, shape&#x3D;(3, 4), dtype&#x3D;int32, numpy&#x3D;
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])&gt;


tf.zeros((2,3,4))

Y &#x3D; tf.constant([[2,1,4,3],[1,2,3,4],[4,3,2,1]])

# æ­£æ€åˆ†å¸ƒ
tf.random.normal(shape&#x3D;[3,4], mean&#x3D;0, stddev&#x3D;1)

# æŒ‰å…ƒç´ ä¹˜æ³•ï¼š
X * Y
# æŒ‰å…ƒç´ æŒ‡æ•°è¿ç®—
Y &#x3D; tf.cast(Y, tf.float32)
tf.exp(Y)


# çŸ©é˜µä¹˜æ³•
Y &#x3D; tf.cast(Y, tf.int32)
tf.matmul(X, tf.transpose(Y))

# æŒ‰åˆ—è¿æ¥
tf.concat([X,Y],axis &#x3D; 0)
# æŒ‰è¡Œè¿æ¥
tf.concat([X,Y],axis &#x3D; 1)

# å¯¹tensorä¸­çš„æ‰€æœ‰å…ƒç´ æ±‚å’Œå¾—åˆ°åªæœ‰ä¸€ä¸ªå…ƒç´ çš„tensorã€‚
tf.reduce_sum(X)

# èŒƒæ•°
X &#x3D; tf.cast(X, tf.float32)
tf.norm(X)

# æ”¯æŒåˆ‡ç‰‡

X[1:3]

X &#x3D; tf.Variable(X)

&lt;tf.Variable &#39;Variable:0&#39; shape&#x3D;(3, 4) dtype&#x3D;float32, numpy&#x3D;
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  9.,  7.],
       [ 8.,  9., 10., 11.]], dtype&#x3D;float32)&gt;

X[1,2].assign(9) # é‡æ–°èµ‹å€¼

# å³ä½¿åƒY &#x3D; X + Yè¿™æ ·çš„è¿ç®—ï¼Œæˆ‘ä»¬ä¹Ÿä¼šæ–°å¼€å†…å­˜ï¼Œç„¶åå°†YæŒ‡å‘æ–°å†…å­˜ã€‚

X &#x3D; tf.Variable(X)
Y &#x3D; tf.cast(Y, dtype&#x3D;tf.float32)

before &#x3D; id(Y)
Y &#x3D; Y + X
id(Y) &#x3D;&#x3D; before

False</code></pre>

<span id="more"></span>

<p>ä¸numpyè½¬æ¢</p>
<pre class="language-none"><code class="language-none">import numpy as np

P &#x3D; np.ones((2,3))
D &#x3D; tf.constant(P)

np.array(D)</code></pre>

<p>tensorä¸numpyè½¬æ¢ </p>
<pre class="language-py" data-language="py"><code class="language-py">import numpy as np
p &#x3D; np.ones((2,3))
d &#x3D; torch.from_numpy(p)

d.numpy()</code></pre>

<p>è‡ªåŠ¨æ±‚æ¢¯åº¦</p>
<p>tensorflow2.0æä¾›çš„GradientTapeæ¥è‡ªåŠ¨æ±‚æ¢¯åº¦ã€‚</p>
<pre class="language-py" data-language="py"><code class="language-py">x &#x3D; tf.reshape(tf.Variable(range(4), dtype&#x3D;tf.float32),(4,1))
x

&lt;tf.Tensor: id&#x3D;10, shape&#x3D;(4, 1), dtype&#x3D;float32, numpy&#x3D;
array([[0.],
       [1.],
       [2.],
       [3.]], dtype&#x3D;float32)&gt;

with tf.GradientTape() as t:
    t.watch(x)
    y &#x3D; 2 * tf.matmul(tf.transpose(x), x)
    
dy_dx &#x3D; t.gradient(y, x)
dy_dx

&lt;tf.Tensor: id&#x3D;30, shape&#x3D;(4, 1), dtype&#x3D;float32, numpy&#x3D;
array([[ 0.],
       [ 4.],
       [ 8.],
       [12.]], dtype&#x3D;float32)&gt;


def f(a):
    b &#x3D; a * 2
    while tf.norm(b) &lt; 1000:
        b &#x3D; b * 2
    if tf.reduce_sum(b) &gt; 0:
        c &#x3D; b
    else:
        c &#x3D; 100 * b
    return c
a &#x3D; tf.random.normal((1,1),dtype&#x3D;tf.float32)
with tf.GradientTape() as t:
    t.watch(a)
    c &#x3D; f(a)
t.gradient(c,a) &#x3D;&#x3D; c&#x2F;a

&lt;tf.Tensor: id&#x3D;201, shape&#x3D;(1, 1), dtype&#x3D;bool, numpy&#x3D;array([[ True]])&gt;</code></pre>

<h4 id="çº¿æ€§å›å½’"><a href="#çº¿æ€§å›å½’" class="headerlink" title="çº¿æ€§å›å½’"></a>çº¿æ€§å›å½’</h4><pre class="language-py" data-language="py"><code class="language-py">import tensorflow as tf
print(tf.__version__)
from matplotlib import pyplot as plt
import random

# ç”Ÿæˆæ•°æ®é›†
num_inputs &#x3D; 2
num_examples &#x3D; 1000
true_w &#x3D; [2, -3.4]
true_b &#x3D; 4.2
features &#x3D; tf.random.normal((num_examples, num_inputs),stddev &#x3D; 1)
labels &#x3D; true_w[0] * features[:,0] + true_w[1] * features[:,1] + true_b
labels +&#x3D; tf.random.normal(labels.shape,stddev&#x3D;0.01)

# è¯»å–æ•°æ®é›†
def data_iter(batch_size, features, labels):
    num_examples &#x3D; len(features)
    indices &#x3D; list(range(num_examples))
    random.shuffle(indices)
    # ä¾æ¬¡è¯»å–ä¸€ä¸ªbatch
    for i in range(0, num_examples, batch_size):
        j &#x3D; indices[i: min(i+batch_size, num_examples)]
        # tf.gatheræŠ½å–å‡ºparamsçš„ç¬¬axisç»´åº¦ä¸Šåœ¨indicesé‡Œé¢æ‰€æœ‰çš„index
        yield tf.gather(features, axis&#x3D;0, indices&#x3D;j), tf.gather(labels, axis&#x3D;0, indices&#x3D;j)

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°

w &#x3D; tf.Variable(tf.random.normal((num_inputs, 1), stddev&#x3D;0.01))
b &#x3D; tf.Variable(tf.zeros((1,)))

# å®šä¹‰æ¨¡å‹
def linreg(X, w, b):
    return tf.matmul(X, w) + b
# æŸå¤±å‡½æ•°
def squared_loss(y_hat, y):
    return (y_hat - tf.reshape(y, y_hat.shape)) ** 2 &#x2F;2
# ä¼˜åŒ–å‡½æ•°
def sgd(params, lr, batch_size, grads):
    &quot;&quot;&quot;Mini-batch stochastic gradient descent.&quot;&quot;&quot;
    for i, param in enumerate(params):
        # æ›´æ–°å‚æ•°
        # assign_sub å˜é‡ ref å‡å» valueå€¼ï¼Œå³ ref &#x3D; ref - value å¹¶èµ‹å€¼
        param.assign_sub(lr * grads[i] &#x2F; batch_size)

# æ¨¡å‹è®­ç»ƒ
lr &#x3D; 0.03
num_epochs &#x3D; 3
net &#x3D; linreg
loss &#x3D; squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        with tf.GradientTape() as t:
            t.watch([w,b])
            l &#x3D; tf.reduce_sum(loss(net(X, w, b), y))
        # è‡ªåŠ¨æ±‚æ¢¯åº¦
        # äºŒç»´çš„å‘é‡ï¼Œåˆ†åˆ«è¡¨ç¤º[w,b]çš„æ¢¯åº¦
        grads &#x3D; t.gradient(l, [w, b])
        sgd([w, b], lr, batch_size, grads)
    train_l &#x3D; loss(net(features, w, b), labels)
    print(&#39;epoch %d, loss %f&#39; % (epoch + 1, tf.reduce_mean(train_l)))</code></pre>

<p>ä½¿ç”¨æ¨¡å—å†™çº¿æ€§å›å½’</p>
<pre class="language-py" data-language="py"><code class="language-py">
import tensorflow as tf

# ç”Ÿæˆæ•°æ®é›†
num_inputs &#x3D; 2
num_examples &#x3D; 1000
true_w &#x3D; [2, -3.4]
true_b &#x3D; 4.2
features &#x3D; tf.random.normal(shape&#x3D;(num_examples, num_inputs), stddev&#x3D;1)
labels &#x3D; true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels +&#x3D; tf.random.normal(labels.shape, stddev&#x3D;0.01)


from tensorflow import data as tfdata

batch_size &#x3D; 10
# å°†è®­ç»ƒæ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ç»„åˆ
dataset &#x3D; tfdata.Dataset.from_tensor_slices((features, labels))
# éšæœºè¯»å–å°æ‰¹é‡
dataset &#x3D; dataset.shuffle(buffer_size&#x3D;num_examples) 
dataset &#x3D; dataset.batch(batch_size)
data_iter &#x3D; iter(dataset) # è¿­ä»£å™¨è¯»å–æ¯ä¸ªbatch

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow import initializers as init

# æ¨¡å‹å®¹å™¨
model &#x3D; keras.Sequential()
model.add(layers.Dense(1, kernel_initializer&#x3D;init.RandomNormal(stddev&#x3D;0.01)))

# æŸå¤±å‡½æ•°
from tensorflow import losses
loss &#x3D; losses.MeanSquaredError()

# ä¼˜åŒ–å™¨
from tensorflow.keras import optimizers
trainer &#x3D; optimizers.SGD(learning_rate&#x3D;0.03)

# è®­ç»ƒæ¨¡å‹
num_epochs &#x3D; 3
for epoch in range(1, num_epochs + 1):
    for (batch, (X, y)) in enumerate(dataset):
        # è®°å½•æ¢¯åº¦
        with tf.GradientTape() as tape:
            l &#x3D; loss(model(X, training&#x3D;True), y)
        # è·å–å˜é‡æ¢¯åº¦
        grads &#x3D; tape.gradient(l, model.trainable_variables)
        # æ›´æ–°å‚æ•°
        # trainerä¸ºä¼˜åŒ–å™¨
        trainer.apply_gradients(zip(grads, model.trainable_variables))
    
    l &#x3D; loss(model(features), labels)
    print(&#39;epoch %d, loss: %f&#39; % (epoch, l))</code></pre>
<p>é€šè¿‡è°ƒç”¨<code>tensorflow.GradientTape</code>è®°å½•åŠ¨æ€å›¾æ¢¯åº¦ï¼Œæ‰§è¡Œ<code>tape.gradient</code>è·å¾—åŠ¨æ€å›¾ä¸­å„å˜é‡æ¢¯åº¦ã€‚é€šè¿‡ <code>model.trainable_variables</code> æ‰¾åˆ°éœ€è¦æ›´æ–°çš„å˜é‡ï¼Œå¹¶ç”¨ <code>trainer.apply_gradients</code> æ›´æ–°æƒé‡ï¼Œå®Œæˆä¸€æ­¥è®­ç»ƒã€‚</p>
<h4 id="æ–‡æœ¬åˆ†æ"><a href="#æ–‡æœ¬åˆ†æ" class="headerlink" title="æ–‡æœ¬åˆ†æ"></a>æ–‡æœ¬åˆ†æ</h4><pre class="language-py" data-language="py"><code class="language-py">import collections
from collections import defaultdict
import os
import random
import tarfile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import preprocessing
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import text_to_word_sequence, one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import sys
import time
import os
sys.path.append(&quot;..&quot;)
import d2lzh_tensorflow2 as d2l
print(tf.test.gpu_device_name())
DATA_ROOT &#x3D; &quot;..&#x2F;..&#x2F;data&quot;

import torchtext.vocab as Vocab

port tensorflow as tf
AUTO &#x3D; tf.data.experimental.AUTOTUNE
physical_devices &#x3D; tf.config.list_physical_devices(&#39;GPU&#39;)
tf.config.experimental.set_memory_growth(physical_devices[0], enable&#x3D;True)

#æ•°æ®æ”¾å…¥dataç›®å½•ä¸‹ï¼Œä»£ç è§£å‹é€Ÿåº¦è¾ƒæ…¢ï¼Œå¦‚æœä¸æƒ³ç”¨ä»£ç è§£å‹ï¼Œä¹Ÿå¯ç›´æ¥æ‰‹åŠ¨è§£å‹ï¼Œè·³è¿‡è¿™ä¸€æ­¥
import os
path &#x3D; os.getcwd()#è¿”å›å½“å‰è¿›ç¨‹çš„å·¥ä½œç›®å½•
a_path &#x3D; os.path.abspath(os.path.join(path, &quot;..&#x2F;..&#x2F;data&#x2F;aclImdb_v1.tar.gz&quot;))
with tarfile.open(a_path, &#39;r&#39;) as f:
    f.extractall(DATA_ROOT)

def read_imdb(folder&#x3D;&#39;train&#39;, data_root&#x3D;&quot;..&#x2F;..&#x2F;data&#x2F;aclImdb&#x2F;&quot;):
    data &#x3D; []
    for label in [&#39;pos&#39;, &#39;neg&#39;]:
        folder_name &#x3D; os.path.join(data_root, folder, label)
        for file in (os.listdir(folder_name)):
            with open(os.path.join(folder_name, file), &#39;rb&#39;) as f:
                review &#x3D; f.read().decode(&#39;utf-8&#39;).replace(&#39;\n&#39;, &#39;&#39;).lower()
                data.append([review, 1 if label &#x3D;&#x3D; &#39;pos&#39; else 0])
    random.shuffle(data)
    return data
# è¯»å–æ•°æ®
train_data, test_data &#x3D; read_imdb(&#39;train&#39;), read_imdb(&#39;test&#39;)

# é¢„å¤„ç†
def get_tokenized_imdb(data):
    &quot;&quot;&quot;
    data: list of [string, label]
    è¿”å›: æ¯ä¸€æ¡è¯„è®ºçš„å•è¯æ‰€ç»„æˆçš„åˆ—è¡¨
    &quot;&quot;&quot;
    def tokenizer(text):
        #åŸºäºç©ºæ ¼è¿›è¡Œåˆ†è¯ï¼Œå¹¶éƒ½è½¬æ¢ä¸ºå°å†™
        return [tok.lower() for tok in text.split(&#39; &#39;)]

    return [tokenizer(review) for review, _ in data]

text &#x3D; get_tokenized_imdb(train_data)

# Counterå¯¹åˆ—è¡¨ä¸­çš„å•è¯è¿›è¡Œè®¡æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—å…¸
counter &#x3D; collections.Counter([tk for st in text for tk in st])
# vocabæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®è¡¨ç¤ºå•è¯ï¼Œå€¼è¡¨ç¤ºå•è¯å‡ºç°çš„é¢‘ç‡
vocab &#x3D; &#123;w: freq for w, freq in counter.most_common() if freq &gt; 5&#125;

def get_vocab_imdb(data):
    tokenized_data &#x3D; get_tokenized_imdb(data)
    #counterå·²ç»åˆ›å»ºäº†ä¸€ä¸ªè¯å…¸,ç»Ÿè®¡äº†æ¯ä¸ªè¯å‡ºç°çš„é¢‘ç‡
    counter &#x3D; collections.Counter([tk for st in tokenized_data for tk in st])
    return Vocab.Vocab(counter, min_freq&#x3D;5)
    #textå€¼ä¿å­˜counterä¸­å‡ºç°é¢‘ç‡å¤§äºç­‰äºäº”çš„è¯
#     text &#x3D; &#123;w: freq for w, freq in counter.most_common() if freq &gt;&#x3D; 5&#125;
#     vocab &#x3D;&#123;index:word for word,index in enumerate(text.keys())&#125;
#     return vocab
    #return Vocab.Vocab(counter, min_freq&#x3D;5)


vocab &#x3D; get_vocab_imdb(train_data)
&#39;# words in vocab:&#39;, len(vocab)

#åœ¨æ­¤å¤„ä½¿ç”¨tensorflow2çš„å¡«å……å‡½æ•°è¿›è¡Œå¡«å……
def preprocess_imdb(data, vocab):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_tensorflow2åŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    max_l &#x3D; 500

    # å°†æ¯æ¡è¯„è®ºé€šè¿‡æˆªæ–­æˆ–è€…è¡¥0ï¼Œä½¿å¾—é•¿åº¦å˜æˆ500
    def pad(x):
        return x[:max_l] if len(x) &gt; max_l else x + [0] * (max_l - len(x))
    
    #tokenized_dataä¸ºä¸€ä¸ªäºŒç»´çš„åˆ—è¡¨,é‡Œé¢æœ‰æˆ‘ä»¬åˆ†å¥½çš„è¯
    tokenized_data &#x3D; get_tokenized_imdb(data)
    
     #å°†æ¯ä¸ªè¯è½¬æ¢ä¸ºè¯ç´¢å¼•å¹¶è¿›è¡Œæˆªæ–­æˆ–è¡¥0
    features &#x3D; tf.Variable([pad([vocab[word] for word in words] ) for words in tokenized_data])
    labels &#x3D; tf.Variable([score for _, score in data])
    return features, labels

data, label &#x3D; preprocess_imdb(train_data, vocab)

batch_size &#x3D; 64

# è®­ç»ƒé›†
train_set &#x3D; (tf.data.Dataset.from_tensor_slices(
    ((preprocess_imdb(train_data, vocab))))
    .repeat()
    .shuffle(2048)
    .batch(batch_size)
    .prefetch(AUTO))
# æµ‹è¯•é›†
test_set &#x3D; (tf.data.Dataset.from_tensor_slices(
    ((preprocess_imdb(test_data, vocab))))
    .shuffle(2048)
    .batch(batch_size)
    .prefetch(AUTO))

#å› ä¸ºtensorflowå¹¶æ²¡æœ‰åƒpytorchï¼Œmxnetå…³äºgloveæ¥å£çš„apiï¼Œæ‰€ä»¥å¿…é¡»è¦é‡å†™ä¸€ä¸ª

def load_embedding_from_disks(glove_filename, with_indexes&#x3D;True):
    &quot;&quot;&quot;
    Read a GloVe txt file. If &#96;with_indexes&#x3D;True&#96;, we return a tuple of two dictionnaries
    &#96;(word_to_index_dict, index_to_embedding_array)&#96;, otherwise we return only a direct 
    &#96;word_to_embedding_dict&#96; dictionnary mapping from a string to a numpy array.
    &quot;&quot;&quot;
    if with_indexes:
        word_to_index_dict &#x3D; dict()
        index_to_embedding_array &#x3D; []
        index_to_word_dict &#x3D; dict()
        word_to_embedding &#x3D; dict()
    else:
        word_to_embedding_dict &#x3D; dict()

    
    with open(glove_filename, &#39;r&#39;,encoding&#x3D;&#39;utf-8&#39;) as glove_file:
        for (i, line) in enumerate(glove_file):
            
            split &#x3D; line.split(&#39; &#39;)
            
            word &#x3D; split[0]
            
            representation &#x3D; split[1:]
            representation &#x3D; np.array(
                [float(val) for val in representation]
            )
            
            if with_indexes:
                word_to_index_dict[word] &#x3D; i
                index_to_word_dict[i] &#x3D; word
                word_to_embedding[word] &#x3D; representation
                index_to_embedding_array.append(representation)
            else:
                word_to_embedding_dict[word] &#x3D; representation

    _WORD_NOT_FOUND &#x3D; [0.0]* len(representation)  # Empty representation for unknown words.
    if with_indexes:
        _LAST_INDEX &#x3D; i + 1
        word_to_index_dict &#x3D; defaultdict(lambda: _LAST_INDEX, word_to_index_dict)
        index_to_embedding_array &#x3D; np.array(index_to_embedding_array + [_WORD_NOT_FOUND])
        return word_to_index_dict, index_to_embedding_array,index_to_word_dict,word_to_embedding
    else:
        word_to_embedding_dict &#x3D; defaultdict(lambda: _WORD_NOT_FOUND)
        return word_to_embedding_dict
    
word_to_index, index_to_embedding, index_to_word,word_to_embedding &#x3D; load_embedding_from_disks(&quot;C:&#x2F;Users&#x2F;HP&#x2F;dive into d2l&#x2F;code&#x2F;chapter10_natural-language-processing&#x2F;embeddings&#x2F; GloVe.6B&#x2F;glove.6B.50d.txt&quot;, with_indexes&#x3D;True)

def get_weights(vocab, word_to_embedding,embedding_dim,word_to_index,index_to_embedding):
    &quot;&quot;&quot;ä»é¢„è®­ç»ƒå¥½çš„vocabä¸­æå–å‡ºwordså¯¹åº”çš„è¯å‘é‡&quot;&quot;&quot;
    embedding_matrix &#x3D; np.zeros((len(vocab), embedding_dim))
#     embedding_matrix &#x3D; np.zeros((len(vocab), embedding_dim))
    for index, word in enumerate(vocab.itos):
        if word in word_to_embedding.keys():
            embedding_matrix[index] &#x3D; index_to_embedding[index]
    return embedding_matrix
embedding_matrix &#x3D; get_weights(vocab,word_to_embedding,50,word_to_index,index_to_embedding)
# net.embedding.set_weights([embedding_matrix])
# net.trainable &#x3D; False

embed_size, num_hiddens, max_len &#x3D; 50, 100, 500
num_epochs &#x3D; 5

model &#x3D; tf.keras.Sequential([
    layers.Embedding(len(vocab), embed_size,weights&#x3D;[embedding_matrix],input_length&#x3D;500),
    layers.Bidirectional(layers.LSTM(num_hiddens)),
    tf.keras.layers.Dense(2,activation&#x3D;&#39;softmax&#39;)
])

model.layers[0].trainable &#x3D; False
model.summary()

model.compile(tf.keras.optimizers.Adam(0.01),
            loss&#x3D;&#39;sparse_categorical_crossentropy&#39;,
            metrics&#x3D;[&#39;sparse_categorical_accuracy&#39;])

model.fit(
    train_set,
    steps_per_epoch&#x3D;data.shape[0]&#x2F;&#x2F;batch_size,
    validation_data&#x3D; test_set,
    epochs&#x3D;5
    )
</code></pre>

<h3 id="tensorflow-1-0"><a href="#tensorflow-1-0" class="headerlink" title="tensorflow 1.0"></a>tensorflow 1.0</h3><p>ä¸€ä¸ªTensorFlowå›¾ç”±ä¸‹é¢å‡ ä¸ªéƒ¨åˆ†ç»„æˆ</p>
<ul>
<li>å ä½ç¬¦å˜é‡ï¼ˆPlaceholderï¼‰ç”¨æ¥æ”¹å˜å›¾çš„è¾“å…¥ã€‚</li>
<li>æ¨¡å‹å˜é‡ï¼ˆModelï¼‰å°†ä¼šè¢«ä¼˜åŒ–ï¼Œä½¿å¾—æ¨¡å‹è¡¨ç°å¾—æ›´å¥½ã€‚</li>
<li>æ¨¡å‹æœ¬è´¨ä¸Šå°±æ˜¯ä¸€äº›æ•°å­¦å‡½æ•°ï¼Œå®ƒæ ¹æ®Placeholderå’Œæ¨¡å‹çš„è¾“å…¥å˜é‡æ¥è®¡ç®—ä¸€äº›è¾“å‡ºã€‚</li>
<li>ä¸€ä¸ªcoståº¦é‡ç”¨æ¥æŒ‡å¯¼å˜é‡çš„ä¼˜åŒ–ã€‚</li>
<li>ä¸€ä¸ªä¼˜åŒ–ç­–ç•¥ä¼šæ›´æ–°æ¨¡å‹çš„å˜é‡ã€‚</li>
<li>å¦å¤–ï¼ŒTensorFlowå›¾ä¹ŸåŒ…å«äº†ä¸€äº›è°ƒè¯•çŠ¶æ€ï¼Œæ¯”å¦‚ç”¨TensorBoardæ‰“å°logæ•°æ®</li>
</ul>
<p>Placeholderæ˜¯ä½œä¸ºå›¾çš„è¾“å…¥ï¼Œæ¯æ¬¡æˆ‘ä»¬è¿è¡Œå›¾çš„æ—¶å€™éƒ½å¯èƒ½ä¼šæ”¹å˜å®ƒä»¬ã€‚</p>
<pre class="language-py" data-language="py"><code class="language-py"># æ•°æ®ç±»å‹è®¾ç½®ä¸ºfloat32ï¼Œå½¢çŠ¶è®¾ä¸º[None, img_size_flat]ï¼ŒNoneä»£è¡¨tensorå¯èƒ½ä¿å­˜ç€ä»»æ„æ•°é‡çš„å›¾åƒï¼Œæ¯å¼ å›¾è±¡æ˜¯ä¸€ä¸ªé•¿åº¦ä¸ºimg_size_flatçš„å‘é‡ã€‚
x &#x3D; tf.placeholder(tf.float32, [None, img_size_flat])
y_true_cls &#x3D; tf.placeholder(tf.int64, [None])

# å˜é‡çš„å½¢çŠ¶æ˜¯[None, num_classes]ï¼Œè¿™ä»£è¡¨ç€å®ƒä¿å­˜äº†ä»»æ„æ•°é‡çš„æ ‡ç­¾ï¼Œæ¯ä¸ªæ ‡ç­¾æ˜¯é•¿åº¦ä¸ºnum_classesçš„å‘é‡ï¼Œæœ¬ä¾‹ä¸­é•¿åº¦ä¸º10ã€‚
y_true &#x3D; tf.placeholder(tf.float32, [None, num_classes])

weights &#x3D; tf.Variable(tf.zeros([img_size_flat, num_classes]))

logits &#x3D; tf.matmul(x, weights) + biases
y_pred &#x3D; tf.nn.softmax(logits)
y_pred_cls &#x3D; tf.argmax(y_pred, axis&#x3D;1)

cross_entropy &#x3D; tf.nn.softmax_cross_entropy_with_logits_v2(logits&#x3D;logits,
                                                           labels&#x3D;y_true)
cost &#x3D; tf.reduce_mean(cross_entropy)
optimizer &#x3D; tf.train.GradientDescentOptimizer(learning_rate&#x3D;0.5).minimize(cost)

correct_prediction &#x3D; tf.equal(y_pred_cls, y_true_cls)
accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# ä¼šè¯
session &#x3D; tf.Session()
# ä¼šå¯¹weightså’Œbiaseså˜é‡åˆå§‹åŒ–
session.run(tf.global_variables_initializer())ã€

batch_size &#x3D; 100
def optimize(num_iterations):
    for i in range(num_iterations):
        # Get a batch of training examples.
        # x_batch now holds a batch of images and
        # y_true_batch are the true labels for those images.
        x_batch, y_true_batch, _ &#x3D; data.random_batch(batch_size&#x3D;batch_size)
        
        # Put the batch into a dict with the proper names
        # for placeholder variables in the TensorFlow graph.
        # Note that the placeholder for y_true_cls is not set
        # because it is not used during training.
        feed_dict_train &#x3D; &#123;x: x_batch,
                           y_true: y_true_batch&#125;

        # Run the optimizer using this batch of training data.
        # TensorFlow assigns the variables in feed_dict_train
        # to the placeholder variables and then runs the optimizer.
        session.run(optimizer, feed_dict&#x3D;feed_dict_train)</code></pre>

<p>ä¼šè¯æ˜¯ç”¨æ¥æ‰§è¡Œå®šä¹‰å¥½çš„è¿ç®—ï¼Œä¼šè¯æ‹¥æœ‰å¹¶ç®¡ç†TensorFlowç¨‹åºè¿è¡Œæ—¶çš„æ‰€æœ‰èµ„æºï¼Œæ‰€æœ‰è®¡ç®—å®Œæˆä¹‹åéœ€è¦å…³é—­ä¼šè¯æ¥å¸®åŠ©ç³»ç»Ÿå›æ”¶èµ„æºã€‚ç”¨æˆ·é€šè¿‡placeholderå®šä¹‰å ä½ç¬¦å¹¶æ„å»ºå®Œæ•´Graphåï¼Œåˆ©ç”¨Sessionå®ä¾‹.runå°†è®­ç»ƒ/æµ‹è¯•æ•°æ®æ³¨å…¥åˆ°å›¾ä¸­ï¼Œé©±åŠ¨ä»»åŠ¡çš„å®é™…è¿è¡Œã€‚</p>
<p>TensorFlowçš„è®¾è®¡å¤„å¤„ä½“ç°äº†ä¸€ä¸ªå»¶åè®¡ç®—çš„æ€æƒ³ã€‚ä»è€Œå¯ä»¥åšæ›´å¤šé™æ€åˆ†æï¼Œå›¾ä¼˜åŒ–ï¼Œä»è€Œè¾¾åˆ°é«˜è¿è¡Œæ•ˆç‡ã€‚è€Œpytorchå›¾åŠ¨æ€æ¯ä¸€æ¬¡å‰å‘æ—¶æ„å»ºgraphï¼Œåå‘æ—¶é”€æ¯ã€‚</p>
</div></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%9B%E5%A4%A7%E4%BB%B6/2021-07-20-IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/" rel="prev" title="IOå¤šè·¯å¤ç”¨"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">IOå¤šè·¯å¤ç”¨</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/2021-07-17-NLP%E4%B8%80%E4%BA%9B%E5%B7%A5%E5%85%B7/" rel="next" title="NLP tools"><span class="post-nav-text">NLP tools</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>å¦‚æœæ‚¨æœ‰ä»»ä½•å…³äºåšå®¢å†…å®¹çš„ç›¸å…³è®¨è®ºï¼Œæ¬¢è¿å‰å¾€ <a href="https://github.com/YunYouJun/yunyoujun.github.io/discussions" target="_blank">GitHub Discussions</a> ä¸æˆ‘äº¤æµã€‚</span><br></div><div id="valine-container"></div><script>Yun.utils.getScript("https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js", () => {
  const valineConfig = {"enable":true,"appId":"K4LElSwpTJaHOOTTU6mNGCyr-gzGzoHsz","appKey":"x3d4Sv6rdTYOECKqkxg9r905","placeholder":"å¡«å†™é‚®ç®±ï¼Œå¯ä»¥æ”¶åˆ°å›å¤é€šçŸ¥å“¦ï½","avatar":null,"pageSize":10,"visitor":false,"highlight":true,"recordIP":false,"enableQQ":true,"meta":["nick","mail","link"],"el":"#valine-container","lang":"zh-cn"}
  valineConfig.path = "/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/2021-07-18-tensorflow/"
  new Valine(valineConfig)
}, window.Valine);</script></div></main><footer class="sidebar-translate" id="footer"><div class="beian"><a rel="noopener" href="https://beian.miit.gov.cn/" target="_blank">èŒICPå¤‡666666å·</a></div><div class="copyright"><span>&copy; 2020 â€“ 2021 </span><a class="with-love" id="animate" target="_blank" rel="noopener" href="https://sponsors.yunyoujun.cn" title="äº‘æ¸¸å›çš„èµåŠ©è€…ä»¬"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></a><span class="author"> larry</span></div><div class="powered"><span>ç”± <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> é©±åŠ¨ v5.4.0</span><span class="footer-separator">|</span><span>ä¸»é¢˜ - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.6.3</span></div><div class="live_time"><span>æœ¬åšå®¢å·²èŒèŒå“’åœ°è¿è¡Œ</span><span id="display_live_time"></span><span class="moe-text">(â—'â—¡'â—)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2019-04-12T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = " " + passDay + " å¤© " + passHour + " å°æ—¶ " + passMinute + " åˆ† " + passSecond + " ç§’";
}
blog_live_time();
</script></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#6200ee" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="æœç´¢"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script defer src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script defer src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script defer src="/js/search/algolia-search.js"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div class="algolia-pagination" id="algolia-pagination"></div></div></div></div><!-- hexo injector body_end start --><script src="/js/tag-common/index.js"></script><!-- hexo injector body_end end --></body></html>